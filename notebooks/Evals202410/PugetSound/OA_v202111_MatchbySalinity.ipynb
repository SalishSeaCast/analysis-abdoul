{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "from salishsea_tools import evaltools as et\n",
    "import datetime as dt\n",
    "import matplotlib as mpl\n",
    "import netCDF4 as nc\n",
    "\n",
    "fs=16\n",
    "mpl.rc('xtick', labelsize=fs)\n",
    "mpl.rc('ytick', labelsize=fs)\n",
    "mpl.rc('legend', fontsize=fs)\n",
    "mpl.rc('axes', titlesize=fs)\n",
    "mpl.rc('axes', labelsize=fs)\n",
    "mpl.rc('figure', titlesize=fs)\n",
    "mpl.rc('font', size=fs)\n",
    "mpl.rc('font', family='sans-serif', weight='normal', style='normal')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('/ocean/atall/MOAD/Obs/SalishCruise_dataProduct_2015to2018_fromKaryn.csv')\n",
    "PATH= '/results2/SalishSea/nowcast-green.202111/'\n",
    "data['dtUTC']=pd.to_datetime(data['dtUTC']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matchData(\n",
    "    data,\n",
    "    filemap,\n",
    "    fdict,\n",
    "    mod_start=None,\n",
    "    mod_end=None,\n",
    "    mod_nam_fmt='nowcast',\n",
    "    mod_basedir='/results2/SalishSea/nowcast-green.202111/',\n",
    "    mod_flen=1,\n",
    "    method='bin',\n",
    "    meshPath=None,\n",
    "    maskname='tmask',\n",
    "    wrapSearch=False,\n",
    "    fastSearch=False,\n",
    "    wrapTol=1,\n",
    "    e3tvar='e3t',\n",
    "    fid=None,\n",
    "    sdim=3,\n",
    "    quiet=False,\n",
    "    preIndexed=False\n",
    "    ):\n",
    "    \"\"\"Given a discrete sample dataset, find match model output\n",
    "\n",
    "    note: only one grid mask is loaded so all model variables must be on same grid; defaults to tmask;\n",
    "        call multiple times for different grids (eg U,W)\n",
    "\n",
    "    :arg data: pandas dataframe containing data to compare to. Must include the following:\n",
    "        'dtUTC': column with UTC date and time\n",
    "        'Lat': decimal latitude\n",
    "        'Lon': decimal longitude\n",
    "        'Z': depth, positive     NOT  required if method='ferry' or sdim=2\n",
    "    :type :py:class:`pandas.DataFrame`\n",
    "\n",
    "    :arg dict filemap: dictionary mapping names of model variables to filetypes containing them\n",
    "\n",
    "    :arg dict fdict: dictionary mapping filetypes to their time resolution in hours\n",
    "\n",
    "    :arg mod_start: first date of time range to match\n",
    "    :type :py:class:`datetime.datetime`\n",
    "\n",
    "    :arg mod_end: end of date range to match (not included)\n",
    "    :type :py:class:`datetime.datetime`\n",
    "\n",
    "    :arg str mod_nam_fmt: naming format for model files. options are 'nowcast' or 'long'\n",
    "        'nowcast' example: 05may15/SalishSea_1h_20150505_20150505_ptrc_T.nc\n",
    "        'long' example: SalishSea_1h_20150206_20150804_ptrc_T_20150427-20150506.nc\n",
    "                    'long' will recursively search subdirectories (to match Vicky's storage style)\n",
    "\n",
    "    :arg str mod_basedir: path to search for model files; defaults to nowcast-green\n",
    "\n",
    "    :arg int mod_flen: length of model files in days; defaults to 1, which is how nowcast data is stored\n",
    "\n",
    "    :arg str method: method to use for matching. options are:\n",
    "\n",
    "        'bin'- return model value from grid/time interval containing observation\n",
    "        'vvlBin' - same as 'bin' but consider tidal change in vertical grid\n",
    "        'vvlZ' - consider tidal change in vertical grid and interpolate in the vertical\n",
    "        'ferry' - match observations to top model layer\n",
    "        'vertNet' - match observations to mean over a vertical range defined by\n",
    "                    Z_upper and Z_lower; first try will include entire cell containing end points\n",
    "                    and use e3t_0 rather than time-varying e3t\n",
    "\n",
    "    :arg str meshPath: path to mesh file; defaults to None, in which case set to:\n",
    "            '/results/forcing/atmospheric/GEM2.5/operational/ops_y2015m01d01.nc' if maskName is 'ops'\n",
    "            '/ocean/eolson/MEOPAR/NEMO-forcing/grid/mesh_mask201702_noLPE.nc' else (SalishSeaCast)\n",
    "\n",
    "    :arg str maskName: variable name for mask in mesh file (check code for consistency if not tmask)\n",
    "                       for ops vars use 'ops'\n",
    "\n",
    "    :arg boolean wrapSearch: if True, use wrapper on find_closest_model_point that assumes\n",
    "                             nearness of subsequent values\n",
    "\n",
    "    :arg int wrapTol: assumed search radius from previous grid point if wrapSearch=True\n",
    "\n",
    "    :arg str e3tvar: name of tgrid thicknesses variable; only for method=interpZe3t, which only works on t grid\n",
    "\n",
    "    :arg Dataset fid: optionally include name of a single dataset when looping is not necessary and all matches come from\n",
    "        a single file\n",
    "\n",
    "    :arg int sdim: optionally enter number of spatial dimensions (must be the same for all variables per call);\n",
    "        defaults to 3; use to match to 2d fields like ssh\n",
    "\n",
    "    :arg boolean quiet: if True, suppress non-critical warnings\n",
    "\n",
    "    :arg boolean preIndexed: set True if horizontal  grid indices already in input dataframe; for\n",
    "               speed; not implemented with all options\n",
    "    \"\"\"\n",
    "    # define dictionaries of mesh lat and lon variables to use with different grids:\n",
    "    lonvar={'tmask':'nav_lon','umask':'glamu','vmask':'glamv','fmask':'glamf'}\n",
    "    latvar={'tmask':'nav_lat','umask':'gphiu','vmask':'gphiv','fmask':'gphif'}\n",
    "\n",
    "    # check that required columns are in dataframe:\n",
    "    if method == 'ferry' or sdim==2:\n",
    "        reqsubset=['dtUTC','Lat','Lon']\n",
    "        if preIndexed:\n",
    "            reqsubset=['dtUTC','i','j']\n",
    "    elif method == 'vertNet':\n",
    "        reqsubset=['dtUTC','Lat','Lon','Z_upper','Z_lower']\n",
    "        if preIndexed:\n",
    "            reqsubset=['dtUTC','i','j','Z_upper','Z_lower']\n",
    "    else:\n",
    "        reqsubset=['dtUTC','Lat','Lon','Z']\n",
    "        if preIndexed:\n",
    "            reqsubset=['dtUTC','i','j','k']\n",
    "    if not set(reqsubset) <= set(data.keys()):\n",
    "\n",
    "        raise Exception('{} missing from data'.format([el for el in set(reqsubset)-set(data.keys())],'%s'))\n",
    "\n",
    "    fkeysVar=list(filemap.keys()) # list of model variables to return\n",
    "    # don't load more files than necessary:\n",
    "    ftypes=list(fdict.keys())\n",
    "    for ikey in ftypes:\n",
    "        if ikey not in set(filemap.values()):\n",
    "            fdict.pop(ikey)\n",
    "    if len(set(filemap.values())-set(fdict.keys()))>0:\n",
    "        print('Error: file(s) missing from fdict:',set(filemap.values())-set(fdict.keys()))\n",
    "    ftypes=list(fdict.keys()) # list of filetypes to containing the desired model variables\n",
    "    # create inverted version of filemap dict mapping file types to the variables they contain\n",
    "    filemap_r=dict()\n",
    "    for ift in ftypes:\n",
    "        filemap_r[ift]=list()\n",
    "    for ikey in filemap:\n",
    "        filemap_r[filemap[ikey]].append(ikey)\n",
    "\n",
    "    # if mod_start and mod_end not provided, use min and max of data datetimes\n",
    "    if mod_start is None:\n",
    "        mod_start=np.min(data['dtUTC'])\n",
    "        print(mod_start)\n",
    "    if mod_end is None:\n",
    "        mod_end=np.max(data['dtUTC'])\n",
    "        print(mod_end)\n",
    "    # adjustments to data dataframe to avoid unnecessary calculations\n",
    "    data=data.loc[(data.dtUTC>=mod_start)&(data.dtUTC<mod_end)].copy(deep=True)\n",
    "    data=data.dropna(how='any',subset=reqsubset) #.dropna(how='all',subset=[*varmap.keys()])\n",
    "    if maskname=='ops':\n",
    "        # set default mesh file for ops data (atmos forcing)\n",
    "        if meshPath==None:\n",
    "            meshPath='/results/forcing/atmospheric/GEM2.5/operational/ops_y2015m01d01.nc'\n",
    "        # load lat, lon, and mask (all ones for ops - no land in sky)\n",
    "        with nc.Dataset(meshPath) as fmesh:\n",
    "            navlon=np.squeeze(np.copy(fmesh.variables['nav_lon'][:,:]-360))\n",
    "            navlat=np.squeeze(np.copy(fmesh.variables['nav_lat'][:,:]))\n",
    "        omask=np.expand_dims(np.ones(np.shape(navlon)),axis=(0,1))\n",
    "        nemops='GEM2.5'\n",
    "    else:\n",
    "        # set default mesh file for SalishSeaCast data\n",
    "        if meshPath==None:\n",
    "            meshPath='/ocean/eolson/MEOPAR/NEMO-forcing/grid/mesh_mask201702_noLPE.nc'\n",
    "        # load lat lon and ocean mask\n",
    "        with nc.Dataset(meshPath) as fmesh:\n",
    "            omask=np.copy(fmesh.variables[maskname])\n",
    "            navlon=np.squeeze(np.copy(fmesh.variables[lonvar[maskname]][:,:]))\n",
    "            navlat=np.squeeze(np.copy(fmesh.variables[latvar[maskname]][:,:]))\n",
    "            if method == 'vertNet':\n",
    "                e3t0=np.squeeze(np.copy(fmesh.variables['e3t_0'][0,:,:,:]))\n",
    "                if maskname != 'tmask':\n",
    "                    print('Warning: Using tmask thickness for variable on different grid')\n",
    "        nemops='NEMO'\n",
    "\n",
    "    # handle horizontal gridding as necessary; make sure data is in order of ascending time\n",
    "    if not preIndexed:\n",
    "        # find location of each obs on model grid and add to data as additional columns 'i' and 'j'\n",
    "        data=et._gridHoriz(data,omask,navlon,navlat,wrapSearch,wrapTol,fastSearch, quiet=quiet,nemops=nemops)\n",
    "        data=data.sort_values(by=[ix for ix in ['dtUTC','Z','j','i'] if ix in reqsubset]) # preserve list order\n",
    "    else:\n",
    "        data=data.sort_values(by=[ix for ix in ['dtUTC','k','j','i'] if ix in reqsubset]) # preserve list order\n",
    "    data.reset_index(drop=True,inplace=True)\n",
    "\n",
    "    # set up columns to accept model values; prepend 'mod' to distinguish from obs names\n",
    "    for ivar in filemap.keys():\n",
    "        data['mod_'+ivar]=np.full(len(data),np.nan)\n",
    "\n",
    "    # create dictionary of dataframes of filename, start time, and end time for each file type\n",
    "    flist=dict()\n",
    "    for ift in ftypes:\n",
    "        flist[ift]=et.index_model_files(mod_start,mod_end,mod_basedir,mod_nam_fmt,mod_flen,ift,fdict[ift])\n",
    "\n",
    "    # call a function to carry out vertical matching based on specified method\n",
    "    if method == 'bin':\n",
    "        data = _binmatch(data,flist,ftypes,filemap_r,omask,maskP,sdim,preIndexed=preIndexed)\n",
    "    elif method == 'ferry':\n",
    "        print('data is matched to shallowest model level')\n",
    "        data = _ferrymatch(data,flist,ftypes,filemap_r,omask,fdict)\n",
    "    elif method == 'vvlZ':\n",
    "        data = _interpvvlZ(data,flist,ftypes,filemap,filemap_r,omask,fdict,e3tvar)\n",
    "    elif method == 'vvlBin':\n",
    "        data= _vvlBin(data,flist,ftypes,filemap,filemap_r,omask,fdict,e3tvar)\n",
    "    elif method == 'vertNet':\n",
    "        data = _vertNetmatch(data,flist,ftypes,filemap_r,omask,e3t0,maskP)\n",
    "    elif method == 'salinity':\n",
    "        print('Matching by salinity...')\n",
    "        data = _salinityMatch(data,flist,ftypes,filemap_r,omask,fdict)\n",
    "    else:\n",
    "        print('option '+method+' not written yet')\n",
    "        return\n",
    "    data.reset_index(drop=True,inplace=True)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _salinityMatch(data, flist, ftypes, filemap_r, omask, fdict):\n",
    "    import xarray as xr\n",
    "    import numpy as np\n",
    "    from tqdm import tqdm\n",
    "\n",
    "    #varname = 'vosaline'\n",
    "    matched_salinities = []\n",
    "\n",
    "    # Find which filetype contains salinity\n",
    "    salinity_var, salinity_ftype = None, None\n",
    "    for ftype in ftypes:\n",
    "        for var in filemap_r[ftype]:\n",
    "            if 'sal' in var.lower():  ## was just 'sal' before\n",
    "                salinity_var = var\n",
    "                salinity_ftype = ftype\n",
    "                break\n",
    "        if salinity_var:\n",
    "            break\n",
    "    if not salinity_var:\n",
    "        raise ValueError(\"No salinity variable found in filemap_r.\")\n",
    "\n",
    "    salinity_files = flist[salinity_ftype]\n",
    "    salinity_files.columns = ['fname', 'start', 'end']\n",
    "\n",
    "    # Cache for xarray datasets\n",
    "    dataset_cache = {}\n",
    "\n",
    "    for idx, row in tqdm(data.iterrows(), total=len(data)):\n",
    "        obs_time = row['dtUTC']\n",
    "        obs_sal = row['Sal (g kg-1)']\n",
    "        j, i = int(row['j']), int(row['i'])\n",
    "        k = None\n",
    "\n",
    "        # Step 1: Find matching salinity depth\n",
    "        for _, mf in salinity_files.iterrows():\n",
    "            if mf['start'] <= obs_time < mf['end']:\n",
    "                fname = mf['fname']\n",
    "                if fname not in dataset_cache:\n",
    "                    dataset_cache[fname] = xr.open_dataset(fname)\n",
    "                ds = dataset_cache[fname]\n",
    "\n",
    "                try:\n",
    "                    # Select time (nearest if needed), then slice j,i\n",
    "                    sel = ds[salinity_var].sel(time_counter=obs_time, method='nearest')\n",
    "                    sal_profile = sel[:, j, i].values  # depth profile\n",
    "\n",
    "                    if np.isnan(sal_profile).all():\n",
    "                        matched_salinities.append(np.nan)\n",
    "                        k = None\n",
    "                    else:\n",
    "                        sal_diff = np.abs(sal_profile - obs_sal)\n",
    "                        k = np.nanargmin(sal_diff)\n",
    "                        matched_salinities.append(sal_profile[k])\n",
    "                except Exception as e:\n",
    "                    print(f\"Error reading salinity at {fname}: {e}\")\n",
    "                    matched_salinities.append(np.nan)\n",
    "                    k = None\n",
    "                break\n",
    "\n",
    "        if k is None:\n",
    "            # Fill all variables with NaN\n",
    "            for ft in ftypes:\n",
    "                for var in filemap_r[ft]:\n",
    "                    data.at[idx, f'mod_{var}'] = np.nan\n",
    "            continue\n",
    "\n",
    "        # Step 2: Grab each variable at (time, k, j, i) using xarray\n",
    "        for ft in ftypes:\n",
    "            var_files = flist[ft]\n",
    "            var_files.columns = ['fname', 'start', 'end']\n",
    "\n",
    "            for _, mf in var_files.iterrows():\n",
    "                if mf['start'] <= obs_time < mf['end']:\n",
    "                    fname = mf['fname']\n",
    "                    if fname not in dataset_cache:\n",
    "                        dataset_cache[fname] = xr.open_dataset(fname)\n",
    "                    ds = dataset_cache[fname]\n",
    "\n",
    "                    for var in filemap_r[ft]:\n",
    "                        try:\n",
    "                            sel = ds[var].sel(time_counter=obs_time, method='nearest')\n",
    "                            val = sel[k, j, i].item()\n",
    "                        except Exception:\n",
    "                            val = np.nan\n",
    "                        data.at[idx, f'mod_{var}'] = val\n",
    "                    break\n",
    "\n",
    "    # Close all xarray datasets\n",
    "    for ds in dataset_cache.values():\n",
    "        ds.close()\n",
    "\n",
    "    data['matched_salinity'] = matched_salinities\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matching by salinity...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 915/915 [01:08<00:00, 13.37it/s]\n"
     ]
    }
   ],
   "source": [
    "matched_data15 = matchData(\n",
    "    data=data,  # your observations DataFrame\n",
    "    filemap={'dissolved_inorganic_carbon':'chem_T','total_alkalinity':'chem_T','dissolved_oxygen':'chem_T',\n",
    "         'votemper':'grid_T','vosaline':'grid_T'\n",
    "        },  # tell it which model variable to match\n",
    "    fdict={'chem_T':1,'grid_T':1},  # model file timestep in hours\n",
    "    mod_start=dt.datetime(2015,1,1),\n",
    "    mod_end=dt.datetime(2015,12,31),\n",
    "    method='salinity')\n",
    "matched_data15.to_csv('/ocean/atall/MOAD/ObsModel/202111/ObsModel_202111_bySalinityKaryn_20150101_20151231.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matching by salinity...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 765/765 [00:52<00:00, 14.49it/s]\n"
     ]
    }
   ],
   "source": [
    "matched_data16 = matchData(\n",
    "    data=data,  # your observations DataFrame\n",
    "    filemap={'dissolved_inorganic_carbon':'chem_T','total_alkalinity':'chem_T','dissolved_oxygen':'chem_T',\n",
    "         'votemper':'grid_T','vosaline':'grid_T'\n",
    "        },  # tell it which model variable to match\n",
    "    fdict={'chem_T':1,'grid_T':1},  # model file timestep in hours\n",
    "    mod_start=dt.datetime(2016,1,1),\n",
    "    mod_end=dt.datetime(2016,12,31),\n",
    "    method='salinity')\n",
    "matched_data16.to_csv('/ocean/atall/MOAD/ObsModel/202111/ObsModel_202111_bySalinityKaryn_20160101_20161231.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matching by salinity...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 916/916 [00:55<00:00, 16.38it/s]\n"
     ]
    }
   ],
   "source": [
    "matched_data17 = matchData(\n",
    "    data=data,  # your observations DataFrame\n",
    "    filemap={'dissolved_inorganic_carbon':'chem_T','total_alkalinity':'chem_T','dissolved_oxygen':'chem_T',\n",
    "         'votemper':'grid_T','vosaline':'grid_T'\n",
    "        },  # tell it which model variable to match\n",
    "    fdict={'chem_T':1,'grid_T':1},  # model file timestep in hours\n",
    "    mod_start=dt.datetime(2017,1,1),\n",
    "    mod_end=dt.datetime(2017,12,31),\n",
    "    method='salinity')\n",
    "matched_data17.to_csv('/ocean/atall/MOAD/ObsModel/202111/ObsModel_202111_bySalinityKaryn_20170101_20171231.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matching by salinity...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1035/1035 [01:01<00:00, 16.84it/s]\n"
     ]
    }
   ],
   "source": [
    "matched_data18 = matchData(\n",
    "    data=data,  # your observations DataFrame\n",
    "    filemap={'dissolved_inorganic_carbon':'chem_T','total_alkalinity':'chem_T','dissolved_oxygen':'chem_T',\n",
    "         'votemper':'grid_T','vosaline':'grid_T'\n",
    "        },  # tell it which model variable to match\n",
    "    fdict={'chem_T':1,'grid_T':1},  # model file timestep in hours\n",
    "    mod_start=dt.datetime(2018,1,1),\n",
    "    mod_end=dt.datetime(2018,12,31),\n",
    "    method='salinity')\n",
    "matched_data18.to_csv('/ocean/atall/MOAD/ObsModel/202111/ObsModel_202111_bySalinityKaryn_20180101_20181231.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
